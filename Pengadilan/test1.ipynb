{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dacd098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import PyPDF2\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af128394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\erwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\erwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\erwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d73517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarizer.py\n",
    "\n",
    "# --- 1. KONFIGURASI ---\n",
    "# Sesuaikan path ini jika struktur folder Anda berbeda\n",
    "PDF_DIR = os.path.join('data_putusan', 'dok_putusan_pdf')\n",
    "REF_DIR = os.path.join('data_putusan', 'referensi_ringkasan')\n",
    "\n",
    "# [cite_start]Compression rates yang akan diuji, sesuai paper [cite: 345]\n",
    "COMPRESSION_RATES = [75, 50, 25]\n",
    "\n",
    "# --- 2. SETUP PUSTAKA ---\n",
    "# [cite_start]Inisialisasi Stemmer dan Stopword Remover dari Sastrawi [cite: 29, 146]\n",
    "stemmer_factory = StemmerFactory()\n",
    "stemmer = stemmer_factory.create_stemmer()\n",
    "\n",
    "stopword_factory = StopWordRemoverFactory()\n",
    "stopword_remover = stopword_factory.create_stop_word_remover()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57934275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. FUNGSI-FUNGSI PREPROCESSING ---\n",
    "\n",
    "def parse_pdf(file_path):\n",
    "    \"\"\"Mengekstrak teks mentah dari file PDF menggunakan PyPDF2.\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(file_path, 'rb') as pdf_file:\n",
    "            reader = PyPDF2.PdfReader(pdf_file)\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() or \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error parsing {os.path.basename(file_path)}: {e}\")\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Menjalankan pipeline preprocessing lengkap sesuai metodologi paper.\n",
    "    \"\"\"\n",
    "    # [cite_start]a. Pembersihan Teks: Menghapus watermark dan spasi berlebih [cite: 132, 134]\n",
    "    text = re.sub(r'(?i)mahkamah agung republik indonesia', '', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text).strip()\n",
    "\n",
    "    # b. [cite_start]Normalisasi Singkatan: Mencegah salah deteksi akhir kalimat [cite: 138, 139]\n",
    "    text = re.sub(r'\\b(Kec)\\.\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(Jln)\\.\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\b(No)\\.\\s*', r'\\1 ', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # c. [cite_start]Segmentasi Kalimat [cite: 136]\n",
    "    original_sentences = sent_tokenize(text, language='indonesian')\n",
    "\n",
    "    processed_sentences = []\n",
    "    for sentence in original_sentences:\n",
    "        # d. [cite_start]Stopwords Removal [cite: 149]\n",
    "        temp_sentence = stopword_remover.remove(sentence.lower())\n",
    "        # e. [cite_start]Stemming [cite: 145]\n",
    "        temp_sentence = stemmer.stem(temp_sentence)\n",
    "        processed_sentences.append(temp_sentence)\n",
    "\n",
    "    return original_sentences, processed_sentences\n",
    "\n",
    "# --- 4. FUNGSI PERINGKASAN & EVALUASI ---\n",
    "\n",
    "def summarize_textrank(processed_sents, original_sents, compression_rate):\n",
    "    \"\"\"Meringkas teks menggunakan algoritma TextRank. \"\"\"\n",
    "    if not processed_sents or not any(processed_sents):\n",
    "        return \"\"\n",
    "\n",
    "    # [cite_start]Representasi kalimat (TF-IDF) [cite: 175]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(processed_sents)\n",
    "    except ValueError:\n",
    "        return \"\" # Terjadi jika semua kalimat kosong setelah preprocessing\n",
    "\n",
    "    # [cite_start]Perhitungan matriks kemiripan (Cosine Similarity) [cite: 178, 182]\n",
    "    sim_matrix = cosine_similarity(tfidf_matrix)\n",
    "    np.fill_diagonal(sim_matrix, 0) # Hapus self-links\n",
    "\n",
    "    # Konversi matriks ke graf dan penerapan algoritma TextRank \n",
    "    nx_graph = nx.from_numpy_array(sim_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "\n",
    "    # Pemilihan kalimat terbaik\n",
    "    summary_ratio = (100 - compression_rate) / 100.0\n",
    "    num_summary_sents = max(1, int(len(original_sents) * summary_ratio))\n",
    "    \n",
    "    ranked_sents = sorted(((scores[i], s) for i, s in enumerate(original_sents)), reverse=True)\n",
    "    top_sents = [s for score, s in ranked_sents[:num_summary_sents]]\n",
    "\n",
    "    # Mengurutkan kembali kalimat ringkasan sesuai urutan asli\n",
    "    summary = sorted(top_sents, key=lambda s: original_sents.index(s))\n",
    "    return \" \".join(summary)\n",
    "\n",
    "def evaluate_summary(system_summary, reference_summary):\n",
    "    \"\"\"\n",
    "    [cite_start]Menghitung Precision, Recall, dan F-measure. [cite: 228]\n",
    "    \"\"\"\n",
    "    system_sents = set(sent_tokenize(system_summary.lower()))\n",
    "    reference_sents = set(sent_tokenize(reference_summary.lower()))\n",
    "\n",
    "    tp = len(system_sents.intersection(reference_sents)) # True Positive \n",
    "    fp = len(system_sents.difference(reference_sents))   # False Positive\n",
    "    fn = len(reference_sents.difference(system_sents))   # False Negative\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0 # Rumus Precision [cite: 232]\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0     # Rumus Recall \n",
    "    f_measure = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0 # Rumus F-measure \n",
    "\n",
    "    return precision, recall, f_measure\n",
    "\n",
    "# --- 5. FUNGSI UTAMA ---\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fungsi utama untuk menjalankan seluruh alur replikasi.\"\"\"\n",
    "    if not os.path.exists(PDF_DIR) or not os.path.exists(REF_DIR):\n",
    "        print(\"‚ùå Error: Pastikan folder 'Dataset/dok_putusan_pdf' dan 'Dataset/referensi_ringkasan' ada.\")\n",
    "        return\n",
    "\n",
    "    pdf_files = sorted([f for f in os.listdir(PDF_DIR) if f.endswith('.pdf')])\n",
    "    \n",
    "    # Inisialisasi dictionary untuk menyimpan hasil\n",
    "    results = {rate: {'precision': [], 'recall': [], 'f_measure': []} for rate in COMPRESSION_RATES}\n",
    "\n",
    "    print(f\"üöÄ Memulai replikasi pada {len(pdf_files)} dokumen...\")\n",
    "\n",
    "    for i, pdf_file in enumerate(pdf_files[:10]):\n",
    "        print(f\"\\n[Dokumen(10) {i+1}/{len(pdf_files)}] Memproses: {pdf_file}\")\n",
    "\n",
    "        # 1. Baca PDF dan Teks Referensi\n",
    "        pdf_path = os.path.join(PDF_DIR, pdf_file)\n",
    "        ref_path = os.path.join(REF_DIR, os.path.splitext(pdf_file)[0] + '.txt')\n",
    "\n",
    "        if not os.path.exists(ref_path):\n",
    "            print(f\"  - Peringatan: File referensi '{os.path.basename(ref_path)}' tidak ditemukan.\")\n",
    "            continue\n",
    "            \n",
    "        raw_text = parse_pdf(pdf_path)\n",
    "        with open(ref_path, 'r', encoding='utf-8') as f:\n",
    "            ref_summary = f.read()\n",
    "\n",
    "        # 2. Preprocessing\n",
    "        original_sents, processed_sents = preprocess_text(raw_text)\n",
    "        print(f\"  - Ditemukan {len(original_sents)} kalimat.\")\n",
    "\n",
    "        # 3. Peringkasan dan Evaluasi untuk setiap compression rate\n",
    "        for rate in COMPRESSION_RATES:\n",
    "            # Peringkasan\n",
    "            system_summary = summarize_textrank(processed_sents, original_sents, rate)\n",
    "            \n",
    "            # Evaluasi\n",
    "            p, r, f1 = evaluate_summary(system_summary, ref_summary)\n",
    "            \n",
    "            # Simpan hasil\n",
    "            results[rate]['precision'].append(p)\n",
    "            results[rate]['recall'].append(r)\n",
    "            results[rate]['f_measure'].append(f1)\n",
    "    \n",
    "    print(\"\\n\\n---\" + \"=\"*50)\n",
    "    print(\"üìä HASIL AKHIR REPLIKASI (Rata-rata dari semua dokumen)\")\n",
    "    print(\"---\" + \"=\"*50)\n",
    "\n",
    "    for rate in COMPRESSION_RATES:\n",
    "        avg_p = np.mean(results[rate]['precision'])\n",
    "        avg_r = np.mean(results[rate]['recall'])\n",
    "        avg_f1 = np.mean(results[rate]['f_measure'])\n",
    "        \n",
    "        print(f\"\\n# Compression Rate: {rate}%\")\n",
    "        print(f\"  - Rata-rata Precision: {avg_p:.2f}\")\n",
    "        print(f\"  - Rata-rata Recall:    {avg_r:.2f}\")\n",
    "        print(f\"  - Rata-rata F-measure: {avg_f1:.2f}\")\n",
    "        \n",
    "    print(\"\\n‚úÖ Replikasi Selesai.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "941d4b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\erwin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7067dd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Memulai replikasi pada 50 dokumen...\n",
      "\n",
      "[Dokumen(10) 1/50] Memproses: doc01.pdf\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/indonesian/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\erwin/nltk_data'\n    - 'c:\\\\Users\\\\erwin\\\\VDBQdrant\\\\nltk_data'\n    - 'c:\\\\Users\\\\erwin\\\\VDBQdrant\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\erwin\\\\VDBQdrant\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\erwin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    119\u001b[39m     ref_summary = f.read()\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# 2. Preprocessing\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m original_sents, processed_sents = \u001b[43mpreprocess_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - Ditemukan \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(original_sents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m kalimat.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# 3. Peringkasan dan Evaluasi untuk setiap compression rate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m     26\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb(No)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms*\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m1 \u001b[39m\u001b[33m'\u001b[39m, text, flags=re.IGNORECASE)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# c. [cite_start]Segmentasi Kalimat [cite: 136]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m original_sentences = \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mindonesian\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m processed_sentences = []\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m original_sentences:\n\u001b[32m     33\u001b[39m     \u001b[38;5;66;03m# d. [cite_start]Stopwords Removal [cite: 149]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\VDBQdrant\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\VDBQdrant\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\VDBQdrant\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\VDBQdrant\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\erwin\\VDBQdrant\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/indonesian/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\erwin/nltk_data'\n    - 'c:\\\\Users\\\\erwin\\\\VDBQdrant\\\\nltk_data'\n    - 'c:\\\\Users\\\\erwin\\\\VDBQdrant\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\erwin\\\\VDBQdrant\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\erwin\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VDBQdrant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
